{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install faker\nfrom keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\nfrom keras.layers import RepeatVector, Dense, Activation, Lambda\nfrom keras.optimizers import Adam\nfrom keras.utils import to_categorical\nfrom keras.models import load_model, Model\nimport keras.backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pickle\nfrom faker import Faker\nimport random\nfrom tqdm import tqdm\nfrom babel.dates import format_date\nimport matplotlib.pyplot as plt\nfrom nltk.translate.bleu_score import sentence_bleu\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Generating Dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from faker import Faker\nfake = Faker()\n\n# We need to seed these guys. For some reason I always use 101\nFaker.seed(101)\nrandom.seed(101)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"FORMATS = ['short', # d/M/YY\n           'medium', # MMM d, YYY\n           'medium',\n           'medium',\n           'long', # MMMM dd, YYY\n           'long',\n           'long',\n           'long',\n           'long',\n           'full', # EEEE, MMM dd, YYY\n           'full',\n           'full',\n           'd MMM YYY', \n           'd MMMM YYY',\n           'd MMMM YYY',\n           'd MMMM YYY',\n           'd MMMM YYY',\n           'd MMMM YYY',\n           'dd/MM/YYY',\n           'EE d, MMM YYY',\n           'EEEE d, MMMM YYY']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for format in FORMATS:\n    print('%s => %s' %(format, format_date(fake.date_object(), format=format, locale='en')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_date():\n    dt = fake.date_object()\n\n    try:\n        date = format_date(dt, format=random.choice(FORMATS), locale='en')\n        human_readable = date.lower().replace(',', '')\n        machine_readable = dt.isoformat()\n\n    except AttributeError as e:\n        return None, None, None\n\n    return human_readable, machine_readable, dt\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"create_dataset(m) will generate our dataset, taking m as the number of samples to create. It returns the dataset as a list, two dictionaries mapping index to character (these are our vocabularies), human and machine, and the inverse mapping, inv_machine, chars to index:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_dataset(m):\n    human_vocab = set()\n    machine_vocab = set()\n    dataset = []\n    \n    for i in tqdm(range(m)):\n        h, m, _ = random_date()\n        if h is not None:\n            dataset.append((h, m))\n            human_vocab.update(tuple(h))\n            machine_vocab.update(tuple(m))\n    \n    # We also add two special chars, <unk> for unknown characters, and <pad> to add padding at the end\n    human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], list(range(len(human_vocab) + 2))))\n    inv_machine = dict(enumerate(sorted(machine_vocab)))\n    machine = {v: k for k, v in inv_machine.items()}\n \n    return dataset, human, machine, inv_machine","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's generate a dataset with 30k samples. That's probably way too much, but it should do a good job"},{"metadata":{"trusted":true},"cell_type":"code","source":"# m = 30000\n# dataset, human_vocab, machine_vocab, inv_machine_vocab = create_dataset(m)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Inspecting the first 10 entries. Remember it contains a list of tuples => (human readable, machine readable):"},{"metadata":{"trusted":true},"cell_type":"code","source":"# dataset[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path to 'TrainSet' file\ntrainSetFile = \"../input/iut-machine-translation/TrainSet.pickle\"\n\n# Openning the file, and load contents\nwith open(trainSetFile, 'rb') as file:\n    trainSet = pickle.load(file)\n\n# Samples\nprint(\"Number of samples in train set:\", len(trainSet), \"\\n\")\nfor i in range(10):\n    print(\"Human Input:\", trainSet[i][0], \"\\t\\tMachine Readable:\", trainSet[i][1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainSet[:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Path to 'ValidationSet' file\nvalidationSetFile = \"../input/iut-machine-translation/ValidationSet.pickle\"\n\n# Openning the file, and load contents\nwith open(validationSetFile, 'rb') as file:\n    validationSet = pickle.load(file)\nprint(\"Number of samples in validation set:\", len(validationSet), \"\\n\")\n\n\n# Path to 'TestSet' file\ntestSetFile = \"../input/iut-machine-translation/TestSet.pickle\"\n\n# Openning the file, and load contents\nwith open(testSetFile, 'rb') as file:\n    testSet = pickle.load(file)\nprint(\"Number of samples in validation set:\", len(testSet))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"human_vocab = {\n    ' ': 0,\n    '/': 1,\n    '۰': 2,\n    '۱': 3,\n    '۲': 4,\n    '۳': 5,\n    '۴': 6,\n    '۵': 7,\n    '۶': 8,\n    '۷': 9,\n    '۸': 10,\n    '۹': 11,\n    'ا': 12,\n    'ب': 13,\n    'پ': 14,\n    'ت': 15,\n    'ث': 16,\n    'ج': 17,\n    'چ': 18,\n    'ح': 19,\n    'خ': 20,\n    'د': 21,\n    'ذ': 22,\n    'ر': 23,\n    'ز': 24,\n    'س': 25,\n    'ش': 26,\n    'ص': 27,\n    'ض': 28,\n    'ط': 29,\n    'ظ': 30,\n    'ع': 31,\n    'غ': 32,\n    'ف': 33,\n    'ق': 34,\n    'ک': 35,\n    'گ': 36,\n    'ل': 37,\n    'م': 38,\n    'ن': 39,\n    'و': 40,\n    'ه': 41,\n    'ی': 42,\n    '0': 43,\n    '1': 44,\n    '2': 45,\n    '3': 46,\n    '4': 47,\n    '5': 48,\n    '6': 49,\n    '7': 50,\n    '8': 51,\n    '9': 52,\n    '‌': 0,\n    '<unk>': 53,\n    '<pad>': 54\n}\n\nhuman_vocab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"machine_vocab = {\n    '-': 0,\n    '0': 1,\n    '1': 2,\n    '2': 3,\n    '3': 4,\n    '4': 5,\n    '5': 6,\n    '6': 7,\n    '7': 8,\n    '8': 9,\n    '9': 10\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inv_machine_vocab = {v: k for k, v in machine_vocab.items()}\ninv_machine_vocab","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\n    X, Y = zip(*dataset)\n    \n    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\n    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\n\n    \n    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\n    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\n\n    return X, np.array(Y), Xoh, Yoh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def string_to_int(string, length, vocab):\n    string = string.replace(',','')\n    \n    if len(string) > length:\n        string = string[:length]\n        \n    rep = list(map(lambda x: vocab.get(x, vocab.get('<unk>')), string))\n    \n    if len(string) < length:\n        rep += [vocab['<pad>']] * (length - len(string))\n    \n    return rep","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"string_to_int('‌', 100, human_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Tx = 50\nTy = 10\nX, Y, Xoh, Yoh = preprocess_data(trainSet, human_vocab, machine_vocab, Tx, Ty)\n\nprint(\"X.shape:\", X.shape)\nprint(\"Y.shape:\", Y.shape)\nprint(\"Xoh.shape:\", Xoh.shape)\nprint(\"Yoh.shape:\", Yoh.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"index = 0\nprint(\"Source date:\", trainSet[index][0])\nprint(\"Target date:\", trainSet[index][1])\nprint()\nprint(\"Source after preprocessing (indices):\", X[index])\nprint(\"Target after preprocessing (indices):\", Y[index])\nprint()\nprint(\"Source after preprocessing (one-hot):\", Xoh[index])\nprint(\"Target after preprocessing (one-hot):\", Yoh[index])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Define Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"repeator = RepeatVector(Tx)\nconcatenator = Concatenate(axis=-1)\ndensor1 = Dense(10, activation = \"tanh\")\ndensor2 = Dense(1, activation = \"relu\")\nactivator = Activation('softmax', name='attention_weights')\ndotor = Dot(axes = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def one_step_attention(a, s_prev):\n    s_prev = repeator(s_prev)\n    concat = concatenator([a, s_prev])\n    e = densor1(concat)\n    energies = densor2(e)\n    alphas = activator(energies)\n    context = dotor([alphas, a])\n    \n    return context","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_a = 32\nn_s = 64\npost_activation_LSTM_cell = LSTM(n_s, return_state = True)\noutput_layer = Dense(len(machine_vocab), activation='softmax')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n    X = Input(shape=(Tx, human_vocab_size))\n    s0 = Input(shape=(n_s,), name='s0')\n    c0 = Input(shape=(n_s,), name='c0')\n    s = s0\n    c = c0\n    \n    outputs = []\n    \n    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\n    \n    for t in range(Ty):\n        context = one_step_attention(a, s)\n        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n        out = output_layer(s)\n        outputs.append(out)\n    \n    model = Model([X, s0, c0], outputs)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mod.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train The Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\nmod.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"s0 = np.zeros((len(trainSet), n_s))\nc0 = np.zeros((len(trainSet), n_s))\noutputs = list(Yoh.swapaxes(0,1))\nmod.fit([Xoh, s0, c0], outputs, epochs=20, batch_size=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"expected = []\nfor t in testSet:\n    expected.append(t[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bleu_scores = []\ntrue = 0\nfalse = 0\nfor example in testSet:\n    source = string_to_int(example[0], Tx, human_vocab)\n    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), source)))\n    source = source.reshape((1, ) + source.shape)\n    prediction = mod.predict([source, s0, c0])\n    prediction = np.argmax(prediction, axis = -1)\n    output = [inv_machine_vocab[int(i)] for i in prediction]\n    bleu_score = sentence_bleu(example[1], ''.join(output))\n    bleu_scores.append(bleu_score)\n    \n    print(\"source:\", example[0])\n    print(\"output:\", ''.join(output))\n    print(\"expected:\", example[1])\n    if ''.join(output) == example[1]:\n        true += 1\n    else:\n        false += 1\n    print(\"bleu_score:\", bleu_score)\n    print(\"----\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"bleu_scores:\", np.average(bleu_scores))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"true / len(testSet)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(\"چهار‌شنبه دی ماه ۲۲ هزار و سیصد و نود و چهار\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}